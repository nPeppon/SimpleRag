{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source_url": "https://www.youtube.com/watch?v=sVcwVQRHIc8", "type": "chunk", "doc_id": "3d2cd776-d9ab-4332-a283-d7b046a8e531", "original_doc_id": "2934a702-1b8a-4c62-a8dc-5dad86053b47"}, "page_content": "so like this is like this is for example they like for prompt temp you're an expert World Knowledge I asked you a question your response should be comprehensive not contradict with the following um and this is kind of where you provide your like original and then step back so here's like some example um questions so like um like uh at year saw the creation of the region where the country is located which region of the country um is the county of of herir related um Janell was born in what country what is janell's personal history so that that's maybe a more intuitive example so it's like you ask a very specific question about like the country someone's born the more abstract question is like just give me the general history of this individual without worrying about that particular um more specific question um so let's actually just walk through how this can be done in practice um so again here's kind of like a a diagram of uh the various approaches um from less abstraction to more abstraction now here is where we're formulating our prompt using a few of the few shot examples from the paper um so again like input um yeah something about like the police perform wful arrests and what what camp members of the police do so like it it basically gives the model a few examples um we basically formulate this into a prompt that's really all going on here again we we repeat um this overall prompt which we saw from the paper your expert World Knowledge your test is to step back and paraphrase a question generate more a generic step back question which is easier to answer here are some examples so it's like a very intuitive prompt so okay let's start with the question what is Task composition for llm agents and we're going to say generate stack question okay so this is pretty intuitive right what is a process of task compos I so like not worrying as much about agents but what is that process of task composition in general and then hopefully that can be independently um retrieved we we can independently retrieve documents related to the stepb back question and in addition retrieve documents related to the the actual question and combine those to produce kind of final answer so that's really all that's going on um and here's the response template where we're Plumbing in the stepback context and our question context and so what we're going to do here is we're going to take our input question and perform retrieval on that we're also going to generate our stepb back question and perform retrieval on that we're going to plumb those into the prompt as here's our very here's our basically uh our prompt Keys normal question step back question um and our overall question again we formulate those as a dict we Plum those into our response prompt um and then we go ahead and attempt to answer our overall question so we're going to run that that's running and okay we have our answer now I want to hop over to Langs Smith and attempt to show you um kind of what that looked like under the hood so let's see let's like go into each of these steps so here was our prompt right you're an expert World Knowledge your test to to step back and paraph as a question um so um here were our few shot prompts and this was our this was our uh stepb question so what is the process of task composition um good from the input what is Tas composition for LM agents we perform retrieval on both what is process composition uh and what is for LM agents we perform both retrievals we then populate our prompt with both uh original question answer and then here's the context retrieve from both the question and the stepb back question here was our final answer so again this is kind of a nice technique um probably depends on a lot of the types of like the type of domain you want to perform retrieval on um but in some domains where for example there's a lot of kind of conceptual knowledge that underpins questions you expect users to ask this stepback approach could be really convenient to automatically formulate a higher level question um to for example try to improve retrieval I can imagine if you're working with like kind of textbooks or like technical documentation where you make independent chapters focused on more highlevel kind of like Concepts and then other chapters on like more detailed uh like implementations this kind of like stepb back approach and independent retrieval could be really helpful thanks hi this is Lance from Lang chain this is the fifth video focused on queer translation in our rack from scratch series we're going to be talking about a technique called hide so again queer translation sits kind of at the front of the overall rag flow um and the objective is to take an input question and translate it in some way that improves retrieval now hide is an interesting approach that takes advantage of a very simple idea the basic rag flow takes a question and embeds it takes a document and embeds it and looks for similarity between an embedded document and embedded question but questions and documents are very different text objects so documents can be like very large chunks taken from dense um Publications or other sources whereas questions are short kind of tur potentially ill worded from users and the intuition behind hide is take questions and map them into document space using a hypothetical document or by generating a hypothetical document um that's the basic intuition and the idea kind of shown here visually is that in principle for certain cases a hypothetical document is closer to a desired document you actually want to retrieve in this you know High dimensional embedding space than the sparse raw input question itself so again it's just kind of means of trans translating raw questions into these hypothetical documents that are better suited for retrieval so let's actually do a Code walkthrough to see how this works and it's actually pretty easy to implement which is really nice so first we're just starting with a prompt and we're using the same notebook that we've used for prior videos we have a blog post on agents r index um so what we're going to do is Define a prompt to generate a hypothetical documents in this case we'll say write a write a paper passage uh to answer a given question so let's just run this and see what happens again we're taking our prompt piping it to to open Ai chck gpte and then using string Opa parer and so here's a hypothetical document section related to our question okay and this is derived of course lm's kind of embedded uh kind of World Knowledge which is you know a sane place to generate hypothetical documents now let's now take that hypothetical document and basically we're going to pipe that into a retriever so this means we're going to fetch documents from our index related to this hypothetical document that's been embedded and you can see we get a few qu a few retrieved uh chunks that are related to uh this hypothetical document that's all we've done um and then let's take the final step where we take those retrieve documents here which we defined and our question we're going to pipe that into this rag prompt and then we're going to run our kind of rag chain right here which you've seen before and we get our answer so that's really it we can go to lsmith and we can actually look at what happened um so here for example this was our final um rag prompt answer the following question based on this context and here is the retrieve documents that we passed in so that part's kind of straightforward we can also look at um okay this is our retrieval okay now this is this is actually what we we generated a hypothetical document here um okay so this is our hypothetical document so we've run chat open AI we generated this passage with our hypothetical document and then we've run retrieval here so this is basically showing hypothetical document generation followed by retrieval um so again here was our passage which we passed in and then here's our retrieve documents from the retriever which are related to the passage content so again in this particular index case it's possible that the input question was sufficient to retrieve these documents in fact given prior examples uh I know that some of these same documents are indeed retrieved just from the raw question but in other context it may not be the case so folks have reported nice performance using Hyde uh for certain domains and the Really convenient thing is that you can take this this document generation prompt you can tune this arbitrarily for your domain of Interest so it's absolutely worth experimenting with it's a it's a need approach uh that can overcome some of the challenges with retrieval uh thanks very much hi this is Lance from Lang chain this is the 10th video in our rack from scratch series focused on routing so we talk through query translation which is the process of taking a question and translating in some way it could be decomposing it using stepback prompting or otherwise but the idea here was take our question change it into a form that's better suited for retrieval now routing is the next step which is basically routing that potentially decomposed question to the right source and in many cases that could be a different database so let's say in this toy example we have a vector store a relational DB and a graph DB the what we redo with routing is we simply route the question based upon the cont of the question to the relevant data source so there's a few different ways to do that one is what we call logical routing in this case we basically give an llm knowledge of the various data sources that we have at our disposal and we let the llm kind of Reason about which one to apply the question to so it's kind of like the the LM is applying some logic to determine you which which data sour for example to to use alternatively you can use semantic routing which is where we take a question we embed it and for example we embed prompts we then compute the similarity between our question and those prompts and then we choose a prompt based upon the similarity so the general idea is in our diagram we talk about routing to for example a different database but it can be very general can be routing to different prompt it can be you know really arbitrarily taking this question and sending it at different places be at different prompts be at different Vector stores so let's walk through the code a little bit so you can see just like before we've done a few pip installs we set up lsmith and let's talk through uh logical routing first so so in this toy example let's say we had for example uh three different docs like we had python docs we had JS docs we had goang docs what we want to do is take a question route it to one of those three so what we're actually doing is we're setting up a data model which is basically going to U be bound to our llm and allow the llm to Output one of these three options as a structured object so you really think about this as like classification classification plus function calling to produce a structured output which is constrained to these three possibilities so the way we do that is let's just zoom in here a little bit we can Define like a structured object that we want to get out from our llm like in this case we want for example you know one of these three data sources to be output we can take this and we can actually convert it into open like open for example function schema and then we actually pass that in and bind it to our llm so what happens is we ask a question our llm invokes this function on the output to produce an output that adheres to the schema that we specify so in this case for example um we output like you know in this toy example let's say we wanted like you know an output to be data source Vector store or SQL database the output will contain a data source object and it'll be you know one of the options we specify as a Json string we also instantiate a parser from this object to parse that Json string to an output like a pantic object for example so that's just one toy example and let's show one up here so in this case again we had our three doc sources um we bind that to our llm so you can see we do with structured output basically under the hood that's taking that object definition turning into function schema and binding that function schema to our llm and we call our prompt you're an expert at routing a user question based on you know programming language um that user referring to so let's define our router here now what we're going to do is we'll ask a question that is python code so we'll call that and now it's done and you see the object we get out is indeed it's a route query object so it's exactly it aderes to this data model we've set up and in this case it's it's it's correct so it's calling this python doc so you can we can extract that right here as a string now once we have this you can really easily set up like a route so this could be like our full chain where we take this router we should defined here and then this choose route function can basically take that output and do something with it so for example if python docs this could then apply the question to like a retriever full of python information uh or JS same thing so this is where you would hook basically that question up to different chains that are like you know retriever chain one for python retriever chain two for JS and so forth so this is kind of like the routing mechanism but this is really doing the heavy lifting of taking an input question and turning into a structured object that restricts the output to one of a few output types that we care about in our like routing problem so that's really kind of the way this all hooks together now semantic outing is actually maybe even a little bit more straightforward based on what we've seen previously so in that case let's say we have two prompts we have a physics prompt we have a math prompt we can embed those prompts no problem we do that here now let's say we have an input question from a user like in this case what is a black hole we pass that through we then apply this runnable Lambda function which is defined right here what we're doing here is we're embedding the question we're Computing similarity between the question and the prompts uh we're taking the most similar and then we're basically choosing the prompt based on that similarity and you can see let's run that and try it out and we're using the physics prompt and there we go black holes region and space so that just shows you kind of how you can use semantic routing uh to basically embed a question embed for example various prompts pick the prompt based on sematic similarity so that really gives you just two ways to do routing one is logical routing with function in uh can be used very generally in this case we applied it to like different coding languages but imagine these could be swapped out for like you know my python uh my like vector store versus My Graph DB versus my relational DB and you could just very simply have some description of what each is and you know then not only will the llm do reasoning but it'll also return an object uh that can be parsed very cleanly to produce like one of a few very specific types which then you can reason over like we did here in your routing function so that kind of gives you the general idea and these are really very useful tools and I encourage you to experiment with them thanks hi this is Lance from Lang chain this is the 11th part of our rag from scratch video series focused on query construction so we previously talked through uh query translation which is the process of taking a question and converting it or translating it into a question that's better optimized for retrieval then we talked about routing which is the process of going taking that question routing it to the right Source be it a given Vector store graph DB um or SQL DB for example now we're going to talk about the process of query construction which is basically taking natural language and converting it into particular domain specific language uh for one of these sources now we're going to talk specifically about the process of going from natural language to uh meditated filters for Vector Stores um the problem statement is basically this let's imagine we had an index of Lang Chain video transcripts um you might want to ask a question give me you know or find find me videos on chat Lang chain published after 2024 for example um the the process of query structuring basically converts this natural language question into a structured query that can be applied to the metadata uh filters on your vector store so most Vector stores will have some kind of meditative filters that can do kind of structur querying on top of uh the chunks that are indexed um so for example this type of query will retrieve all chunks uh that talk about the topic of chat Lang chain uh published after the date 2024 that's kind of the problem statement and to do this we're going to use function calling um in this case you can use for example open AI or other providers to do that and we're going to do is at a high level take the metadata fields that are present in our Vector store and divide them to the model as kind of information and the model then can take those and produce queries that adhere to the schema provided um and then we can parse those out to a structured object like a identic object which again which can then be used in search so that's kind of the problem statement and let's actually walk through code um so here's our notebook which we've kind of gone through previously and I'll just show you as an example let's take a example YouTube video and let's look at the metadata that you get with the transcript so you can see you get stuff like description uh URL um yeah publish date length things like that now let's say we had an index that had um basically a that had a number of different metadata fields and filters uh that allowed us to do range filtering on like view count publication date the video length um or unstructured search on contents and title so those are kind of like the imagine we had an index that had uh those kind of filters available to us what we can do is capture that information about the available filters in an object so we're calling that this tutorial search object kind of encapsulates that information about the available searches that we can do and so we basically enumerate it here content search and title search or semantic searches that can be done over those fields um and then these filters then are various types of structure searches we can do on like the length um The View count and so forth and so we can just kind of build that object now we can set this up really easily with a basic simple prompt that says you know you're an expert can bring natural language into database queries you have access to the database tutorial videos um given a question return a database query optimize retrieval so that's kind of it now here's the key point though when you call this LM with structured output you're binding this pantic object which contains all the information about our index to the llm which is exactly what we talked about previously it's really this process right here you're taking this object you're converting it to a function schema for example open AI you're binding that to your model and then you're going to be able to get um structured object out versus a Json string from a natural language question which can then be parsed into a pantic object which you get out so that's really the flow and it's taking advantage of function calling as we said so if we go back down we set up our query analyzer chain right here now let's try to run that just on a on a purely semantic input so rag from scratch let's run that and you can see this just does like a Content search and a title search that's exactly what you would expect now if we pass a question that includes like a date filter let's just see if that would work and there we go so you kind of still get that semantic search um but you also get um search over for example publish date earliest and latest publish date kind of as as you would expect let's try another one here so videos focus on the topic of chat Lang chain they're published before 2024 this is just kind of a rewrite of this question in slightly different way using a different date filter and then you can see", "type": "Document"}}