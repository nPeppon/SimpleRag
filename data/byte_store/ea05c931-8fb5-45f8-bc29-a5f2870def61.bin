{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source": "https://lilianweng.github.io/posts/2024-02-05-human-data-quality/", "title": "Thinking about High-Quality Human Data | Lil'Log", "description": "[Special thank you to Ian Kivlichan for many useful pointers (E.g. the 100+ year old Nature paper \u201cVox populi\u201d) and nice feedback. \ud83d\ude4f ]\nHigh-quality data is the fuel for modern data deep learning model training. Most of the task-specific labeled data comes from human annotation, such as classification task or RLHF labeling (which can be constructed as classification format) for LLM alignment training. Lots of ML techniques in the post can help with data quality, but fundamentally human data collection involves attention to details and careful execution.", "language": "en", "source_url": "https://lilianweng.github.io/posts/2024-02-05-human-data-quality/", "type": "chunk", "doc_id": "ea05c931-8fb5-45f8-bc29-a5f2870def61", "original_doc_id": "5919f85a-dbf0-4b88-a76e-25d84dc4ae48"}, "page_content": "Fig. 7. Illustration of how jury learning works. (Image source: Gordon et al. 2022)\nThe jury learning model is a DCN (Deep & Cross network) , commonly for recommendation use case,  that is jointly trained to learn comment embedding, annotator embedding and group (annotator\u2019s characteristics) embedding. The text content is processed by a pre-trained BERT, which is also jointly fine-tuned but for a shorter period to avoid overfitting.\n\nFig. 8. DCN model architecture for jury learning. (Image source: Gordon et al. 2022)\nTheir experiment runs on the toxicity diversity dataset and compares jury learning with a baseline model which is a fine-tuned BERT to predict individual annotator\u2019s label without using metadata. Performance is measured in MAE (mean absolute error). Jury learning consistently outperforms the annotator-agnostic baseline on the full test set as well as each group segment.\n\nFig. 9. Experiment results comparing an annotator-agnostic baseline with jury learning. (Image source: Gordon et al. 2022)\nData Quality \u2194 Model Training#\nOnce a dataset is constructed, many methods can help identify mislabels according to the training dynamics. Note that we only focus on methods to find and exclude data points with potentially incorrect labels, not about how to train a model with noisy data.\nInfluence Functions#\nInfluence functions is a classic technique from robust statistics (Hampel, 1974) to measure the effect of training data points by describing how the model parameters change as we upweight a training point by an infinitesimal amount. Koh & Liang (2017) introduced the concept to be applied to deep neural networks.\nGiven $n$ data samples in the train set, $z_i = (x_i, y_i)$ for $i =1, \\dots, n$, The model parameter $\\theta$ is optimized to minimize a loss: $\\hat{\\theta} = \\arg\\min_{\\theta \\in \\Theta} \\frac{1}{n}\\sum_{i=1}^n \\mathcal{L}(z_i, \\theta)$. The change of model parameters after we remove a single data point $z$ is denoted as $\\hat{\\theta}_{-z} - \\hat{\\theta}$ where $\\hat{\\theta}_{-z} = \\arg\\min_{\\theta \\in \\Theta} \\frac{1}{n} \\sum_{z_i \\neq z} \\mathcal{L}(z_i, \\theta)$. However, computing this literally for every sample is too expensive. One way to approximate this is to compute the parameter change given a small upweight $\\epsilon$ on $z$. By definition, the influence of upweighting $z$ by $\\epsilon$ is given by:\n\n$$\n\\mathcal{I}_{\\text{up,params}}(z) = \\frac{d\\hat{\\theta}_{\\epsilon,z}}{d\\epsilon}\\bigg\\vert_{\\epsilon=0}=-\\mathbf{H}^{-1}_{\\hat{\\theta}} \\nabla_\\theta \\mathcal{L}(z, \\hat{\\theta})\n$$\n\nwhere $\\hat{\\theta}_{\\epsilon,z} = \\arg\\min_{\\theta \\in \\Theta} \\frac{1}{n}\\sum_{i=1}^n \\mathcal{L}(z_i, \\theta) + \\epsilon L(z, \\theta)$ and $\\mathbf{H}^{-1}_{\\hat{\\theta}} = \\frac{1}{n}\\sum_{i=1}^n \\nabla^2_\\theta \\mathcal{L}(z_i, \\hat{\\theta})$.\nRemoving a data point $x$ is equivalent to upweighting it by $\\epsilon = -\\frac{1}{n}$ and therefore $\\hat{\\theta}_{-z} - \\hat{\\theta} \\approx -\\frac{1}{n} \\mathcal{I}_{\\text{up,params}}(z)$.\nThe influence of upweighting $z$ on the loss at a test point $z_\\text{test}$ is given by applying the chain rule:\n\n$$\n\\begin{aligned}\n\\mathcal{I}_{\\text{up,loss}}(z, z_\\text{test}) \n&= \\frac{d \\mathcal{L}(z_\\text{test}, \\hat{\\theta}_{\\epsilon,z})}{d\\epsilon}\\bigg\\vert_{\\epsilon=0} \\\\\n&= \\nabla_\\theta \\mathcal{L}(z_\\text{test}, \\hat{\\theta})^\\top \\frac{d \\hat{\\theta}_{\\epsilon,z}}{d\\epsilon}\\bigg\\vert_{\\epsilon=0} \\\\\n&= - \\nabla_\\theta \\mathcal{L}(z_\\text{test}, \\hat{\\theta})^\\top \\mathbf{H}^{-1}_{\\hat{\\theta}} \\nabla_\\theta \\mathcal{L}(z, \\hat{\\theta})\n\\end{aligned}\n$$\n\nUsing the influence function we can measure the effect of a single data point on model parameters and loss function in closed forms. It can help approximate leave-one-out retraining without actually running all the retraining. To identify mislabeled data, we can measure $\\mathcal{I}_\\text{up,loss}(z_i, z_i)$, approximating the prediction error on $z_i$ if $z_i$ is removed from the training set.\n\nFig. 10. Influence functions values match leave-one-out training results on 10-class MNIST. (Image source: Kohn & Liang, 2017)\nGiven the closed form, influence functions is still hard to be scaled up because the inverse Hessian vector product is hard to compute. Grosse et al. (2023) experimented with the EK-FAC (Eigenvalue-corrected Kronecker-Factored Approximate Curvature; George et al. 2018) approximation instead.\nPrediction Changes during Training#\nAnother branch of methods are to track the changes of model prediction during training to identify cases which seem hard to be learned. Data Maps (Swayamdipta et al. 2020) tracks two attributes of model behavior dynamics during training to analyze the quality of dataset:\n\nConfidence: The model\u2019s confidence in the true label, defined as the mean model probability of the true label across epochs. They also used a coarse-grained metric, \u201ccorrectness\u201d, defined as the fraction of times when the model predicts the correct label across epochs.\nVariability: The variation of the confidence, defined as the standard deviation of model probability of the true label across epochs.\n\n\nFig. 11. Data map for SNLI training set, based on a RoBERTa classifier. (Image source: Swayamdipta et al. 2020)\nHard-to-learn (low confidence, low variability) samples are more likely to be mislabeled. They ran an experiment on WinoGrande dataset with 1% flipped label data. After retraining, flipped instances move to the lower confidence and slightly higher variability regions, indicating that the hard-to-learn regions contains mislabeled samples. Given this, we can train a classifier on equal numbers of label flipped and clean samples using only the confidence score (unsure why the paper didn\u2019t use both confidence and variability as features). This simple noise classifier then can be used on the original dataset to identify potentially mislabeled instances.\n\nFig. 12. Data points originally with high confidence and low variability scores moved to low confidence, slightly higher variability regions after labels get flipped. (Image source: Swayamdipta et al. 2020)\nHowever, we should not consider all hard-to-learn samples to be incorrect. In fact, the paper hypothesizes that ambiguous (high variability) and hard-to-learn (low confidence, low variability) samples are more informative for learning. Experiments showed that they are good for OOD generalization, giving better results on OOD eval, even in comparison to 100% training set.\nTo investigate whether neural networks have a tendency to forget previously learned information, Mariya Toneva et al. (2019) designed an experiment: They track the model prediction for each sample during the training process and count the transitions for each sample from being classified correctly to incorrectly or vice-versa. Then samples can be categorized accordingly,\n\nForgettable (redundant) samples: If the class label changes across training epochs.\nUnforgettable samples: If the class label assignment is consistent across training epochs. Those samples are never forgotten once learned.\n\nThey found that there are a large number of unforgettable examples that are never forgotten once learnt. Examples with noisy labels or images with \u201cuncommon\u201d features (visually complicated to classify) are among the most forgotten examples. The experiments empirically validated that unforgettable examples can be safely removed without compromising model performance.\nIn the implementation, the forgetting event is only counted when a sample is included in the current training batch; that is, they compute forgetting across presentations of the same example in subsequent mini-batches. The number of forgetting events per sample is quite stable across different seeds and forgettable examples have a small tendency to be first-time learned later in the training. The forgetting events are also found to be transferable throughout the training period and between architectures.\nPleiss, et al. (2020) developed a method named AUM (Area under the Margin) to spot wrong labels based on such an assumption: Say, a BIRD image is mistakenly marked as DOG. The gradient update would encourage generalization from other BIRD images to this BIRD image, while the DOG label provides an incorrect supervised signal to encourage the update to go another way. Hence, there exists tension between generalization and (wrong) prediction in gradient update signals.\nGiven a classification dataset $(\\mathbf{x}, y) \\in \\mathcal{D}_\\text{train}$, let $z^{(t)}_i(\\mathbf{x}) \\in \\mathbb{R}$ be the logit corresponding to class $i$ at epoch $t$. The margin at epoch $t$ is the difference between the assigned logit and the next largest logit:\n\n$$\nM^{(t)}(\\mathbf{x}, y) = z_y^{(t)}(\\mathbf{x}) - \\max_{i \\neq y} z^{(t)}_i(\\mathbf{x}),\\quad\n\\text{AUM}(\\mathbf{x}, y) = \\frac{1}{T} \\sum^T_{t=1} M^{(t)}(\\mathbf{x}, y)\n$$\n\nA negative margin indicates a wrong prediction and a large positive margin suggests high confidence in a correct prediction. The hypothesis is that mislabeled samples would have a smaller margin than correct samples due to the tension of generalization via SGD triggered by other samples.\nIn order to determine the threshold, they insert fake data, named \u201cthreshold samples\u201d, to determine the threshold:\n\nCreate a subset of threshold samples $\\mathcal{D}_\\text{thr}$.  If there are $N$ training samples for $C$ classes, we randomly sample $N/(C+1)$ samples and switch all their labels to a fake new class $C+1$.\nMerge threshold samples into the original dataset: $\\mathcal{D}\u2019 = { (\\mathbf{x}, C+1): \\mathbf{x} \\in \\mathcal{D}_\\text{thr}} \\cup (\\mathcal{D} \\setminus\\mathcal{D}_\\text{thr})$;\nTrain the model on $\\mathcal{D}\u2019$ and measure AUM of all the data;\nCompute the threshold $\\alpha$ as the 99th percentile of AUM of threshold samples;\nIdentify mislabeled data using $\\alpha$ a threshold: ${(\\mathbf{x}, y) \\in \\mathcal{D} \\setminus \\mathcal{D}_\\text{thr}: \\text{AUM}_{\\mathbf{x}, y} \\leq \\alpha}$\n\n\nFig. 13. How the AUM of threshold samples help separate out mislabeled samples. (Image source: Pleiss et al. 2020)\n\nFig. 14. Test error on CIFAR 10/100 with randomly mislabeled samples, comparing different methods for data filter or noisy data training. (Image source: Pleiss et al. 2020)\nNoisy Cross-Validation#\nThe NCV (Noisy Cross-Validation) method (Chen et al. 2019) divides the dataset into half at random, and then identifies data samples as \u201cclean\u201d if its label matches the predicted label provided by the model that is only trained on the other half of the dataset. Clean samples are expected to be more trustworthy. INCV (Iterative Noisy Cross-Validation) runs NCV iteratively where more clean samples are added into the trusted candidate set $\\mathcal{C}$ and more noisy samples are removed.\n\nFig. 15. Algorithm of INCV (iterative noisy cross-validation). (Image source: Chen et al. 2019)\nCitation#\nCited as:\n\nWeng, Lilian. (Feb 2024). \u201cThinking about High-Quality Human Data\u201d. Lil\u2019Log. https://lilianweng.github.io/posts/2024-02-05-human-data-quality/.\n\nOr\n@article{weng2024humandata,\n  title   = \"Thinking about High-Quality Human Data\",\n  author  = \"Weng, Lilian\",\n  journal = \"lilianweng.github.io\",\n  year    = \"2024\",\n  month   = \"Feb\",\n  url     = \"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\"\n}\nReferences#\n[1] Francis Galton \u201cVox populi\u201d  Nature 75, 450-451 (1907).\n[2] Sambasivan et al. \u201cEveryone wants to do the model work, not the data work\u201d: Data Cascades in High-Stakes AI\" CHI 2021\n[3] Chris Callison-Burch. \u201cFast, Cheap, and Creative: Evaluating Translation Quality Using Amazon\u2019s Mechanical Turk\u201d EMNLP 2009\n[4] Rottger et al. \u201cTwo Contrasting Data Annotation Paradigms for Subjective NLP Tasks\u201d NAACL 2022.\n[5] Aroyo & Welty \u201cTruth Is a Lie: Crowd Truth and the Seven Myths of Human Annotation\u201d AI Magazine\u00a036.1: 15-24 (2015).\n[6] Hovy et al. \u201cLearning Whom to Trust with MACE\u201d NAACL-HLT 2013.\n[7] Wang et al. \u201cAll that Agrees Is Not Gold: Evaluating Ground Truth Labels and Dialogue Content for Safety\u201d 2023.\n[8] Zhang et al. \u201cA Taxonomy of Rater Disagreements: Surveying Challenges & Opportunities from the Perspective of Annotating Online Toxicity\u201d arXiv preprint arXiv:2311.04345\u00a0(2023).\n[9] Davani et al. \u201cDealing with disagreements: Looking beyond the majority vote in subjective annotations\u201d ACL 2022.\n[10] Gordon et al. \u201cJury Learning: Integrating Dissenting Voices into Machine Learning Models\u201d CHI 2022.\n[11] Gordon et al. \u201cThe Disagreement Deconvolution: Bringing Machine Learning Performance Metrics In Line With Reality\u201d CHI 2021\n[12] Daniel et al. 2018 \u201cQuality Control in Crowdsourcing: A Survey of Quality Attributes, Assessment Techniques, and Assurance Actions\u201d ACM Computing Surveys (CSUR), 51(1), 1-40 (2018).\n[13] Koh & Liang. \u201cUnderstanding Black-box Predictions via Influence Functions\u201d ICML 2017.\n[14] Grosse et al. \u201cStudying Large Language Model Generalization with Influence Functions\u201d arXiv preprint arXiv:2308.03296 (2023).\n[15] Swayamdipta et al. \u201cDataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics\u201d EMNLP 2020.\n[16] Toneva, et al. \u201cAn Empirical Study of Example Forgetting during Deep Neural Network Learning\u201d ICLR 2019.\n[17] Pleiss, et al.  \u201cIdentifying Mislabeled Data using the Area Under the Margin Ranking\u201d NeuriPS 2020.\n[18] Chen et al. \u201cUnderstanding and utilizing deep neural networks trained with noisy labels\u201d ICML 2019.\n\n\n\ndata\ndata-quality\nhuman-ai\n\n\n\n\u00ab \n\nDiffusion Models for Video Generation\n\n\n \u00bb\n\nAdversarial Attacks on LLMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a9 2024 Lil'Log\n\n        Powered by\n        Hugo &\n        PaperMod", "type": "Document"}}