{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source_url": "https://www.youtube.com/watch?v=sVcwVQRHIc8", "type": "chunk", "doc_id": "156678e8-9c7e-44db-b816-34c1a2e83f11", "original_doc_id": "2934a702-1b8a-4c62-a8dc-5dad86053b47"}, "page_content": "of this whole flow um now if we go to retrieve um then we're going to grade so that's it um and you know it follows kind of as you can see here that's really it uh so it's just nice to draw the these diagrams out first and then it's pretty quick to implement each node and each Edge just as a function and then stitch them together in a graph just like I show here and of course we'll make sure this code's publ so you can use it as a reference um so there we go now let's try a few a few different test questions so like what player the Bears to draft and NFL draft right let's have a look at that and they should print everything it's doing as we go so okay this is important route question it just decides to route to web search that's good it doesn't go to our Vector store this is a current event not related to our Vector store at all it goes to web search um and then it goes to generate so that's what we'd expect so basically web search goes through to generate um and we check hallucinations Generations ground the documents we check generation versus question the generation addresses the question the Chicago Bears expected to draft Caleb Williams that's right that's that's the consensus so cool that works now let's ask a question related to our Vector store what are the types of agent memory we'll kick this off so we're routing okay we're routing to rag now look how fast this is that's really fast so we basically whip through that document grading determine they're all relevant uh we go to decision to generate um we check hallucinations we check answer versus question and there are several types of memory stored in the human brain memory can also be stored in G of Agents you have LM agents memory stream retrieval model and and reflection mechanism so it's representing what's captured on the blog post pretty reasonably now let me show you something else is kind of nice I can go to Langs Smith and I can go to my projects we create this new project coher adaptive rag at the start and everything is actually logged there everything we just did so I can open this up and I can actually just kind of look through all the stages of my Lang graph to here's my retrieval stage um here's my grade document stage and we can kind of audit the grading itself we kind of looked at this one by one previously but it's actually pretty nice we can actually audit every single individual document grade to see what's happening um we can basically go through um to this is going to be one of the other graders here um yep so this is actually going to be the hallucination grading right here uh and then this is going to be the answer grading right here so that's really it you can kind of walk through the entire graph you can you can kind of study what's going on um which is actually very useful so it looks like this worked pretty well um and finally let's just ask a question that should go to that fallback uh path down at the bottom like not related at all to our Vector store current events and yeah hello I'm doing well so it's pretty neat we've seen in maybe 15 minutes we've from scratch built basically a semi- sophisticated rag system that has agentic properties we've done in Lang graph we've done with coher uh command R you can see it's pretty darn fast in fact we can go to Langs Smith and look at so this whole thing took 7 seconds uh that is not bad let's look at the most recent one so this takes one second so the fallback mechanism to the LM is like 1 second um the let's just look here so 6 seconds for the initial uh land graph so this is not bad at all it's quite fast it done it does quite a few different checks we do routing uh and then we have kind of a bunch of nice fallback behavior and inline checking uh for both relevance hallucinations and and answer uh kind of groundedness or answer usefulness so you know this is pretty nice I definitely encourage you to play with a notebook command R is a really nice option for this due to the fact that is tool use routing uh small and quite fast and it's really good for Rags it's a very nice kind of uh a very nice option for workflows like this and I think you're going to see more and more of this kind of like uh adaptive or self-reflective rag um just because this is something that a lot systems can benefit from like a a lot of production rack systems kind of don't necessarily have fallbacks uh depending on for example like um you know if the documents retrieved are not relevant uh if the answer contains hallucinations and so forth so this opportunity to apply inline checking along with rag is like a really nice theme I think we're going to see more and more of especially as model inference gets faster and faster and these checks get cheaper and cheaper to do kind of in the inference Loop now as a final thing I do want to bring up the a point about you know we've shown this Lang graph stuff what about agents you know how do you think about agents versus Lang graph right and and I think the way I like to frame this is that um Lang graph is really good for um flows that you have kind of very clearly defined that don't have like kind of open-endedness but like in this case we know the steps we want to take every time we want to do um basically query analysis routing and then we want to do a three grading steps and that's it um Lang graph is really good for building very reliable flows uh it's kind of like putting an agent on guard rails and it's really nice uh it's less flexible but highly reliable and so you can actually use smaller faster models with langra so that's the thing I like about we saw here command R 35 billion parameter model works really well with langra quite quick we' were able to implement a pretty sophisticated rag flow really quickly 15 minutes um in time is on the order of like less than you know around 5 to 6 seconds so so pretty good right now what about agents right so I think Agents come into play when you want more flexible workflows you don't want to necessarily follow a defined pattern a priori you want an agent to be able to kind of reason and make of open-end decisions which is interesting for certain like long Horizon planning problems you know agents are really interesting the catch is that reliability is a bit worse with agents and so you know that's a big question a lot of people bring up and that's kind of where larger LMS kind of come into play with a you know there's been a lot of questions about using small LMS even open source models with agents and reliabilities kind of continuously being an issue whereas I've been able to run these types of land graphs with um with uh like mraw or you know command R actually is open weights you can run it locally um I've been able to run them very reproducibly with open source models on my laptop um so you know I think there's a tradeoff and Comm actually there's a new coher model coming out uh believe command R plus which uh is a larger model so it's probably more suitable for kind of more open-ended agentic use cases and there's actually a new integration with Lang chain that support uh coher agents um which is quite nice so I think it's it's worth experimenting for certain problems in workflows you may need more open-ended reasoning in which case use an agent with a larger model otherwise you can use like Lang graph for more uh a more reliable potential but con strain flow and it can also use smaller models faster LMS so those are some of the trade-offs to keep in mind but anyway encourage you play with a notebook explore for yourself I think command R is a really nice model um I've also been experimenting with running it locally with AMA uh currently the quantise model is like uh two bit quantise is like 13 billion uh or so uh yeah 13 gigs it's it's a little bit too large to run quickly locally for me um inference for things like rag we're on the order of 30 seconds so again it's not great for a live demo but it does work it is available on a llama so I encourage you to play with that I have a Mac M2 32 gig um so you know if I if you're a larger machine then it absolutely could be worth working with locally so encourage you to play with that anyway hopefully this was useful and interesting I think this is a cool paper cool flow um coher command R is a nice option for these types of like routing uh it's quick good with Lang graph good for rag good for Tool use so you know have a have a look and uh you know reply anything uh any feedback in the comments thanks hi this is Lance from Lang chain this is a talk I gave at two recent meetups in San Francisco called is rag really dead um and I figured since you know a lot of people actually weren't able to make those meetups uh I just record this and put this on YouTube and see if this is of interest to folks um so we all kind of recognize that Contex windows are getting larger for llms so on the x-axis you can see the tokens used in pre-training that's of course you know getting larger as well um proprietary models are somewhere over the two trillion token regime we don't quite know where they sit uh and we've all the way down to smaller models like 52 trained on far fewer tokens um but what's really notable is on the y axis you can see about a year ago da the art models were on the order of 4,000 to 8,000 tokens and that's you know dozens of pages um we saw Claude 2 come out with the 200,000 token model earlier I think it was last year um gbd4 128,000 tokens now that's hundreds of pages and now we're seeing Claud 3 and Gemini come out with million token models so this is hundreds to thousands of pages so because of this phenomenon people have been kind of wondering is rag dead if you can stuff you know many thousands of pages into the context window llm why do you need a reteval system um it's a good question spoke sparked a lot of interesting debate on Twitter um and it's maybe first just kind of grounding on what is rag so rag is really the process of reasoning and retrieval over chunks of of information that have been retrieved um it's starting with you know documents that are indexed um they're retrievable through some mechanism typically some kind of semantic similarity search or keyword search other mechanisms retriev docs should then pass to an llm and the llm reasons about them to ground response to the question in the retrieve document so that's kind of the overall flow but the important point to make is that typically it's multiple documents and involve some form of reasoning so one of the questions I asked recently is you know if long condex llms can replace rag it should be able to perform you know multia retrieval and reasoning from its own context really effectively so I teamed up with Greg Cameron uh to kind of pressure test this and he had done some really nice needle the Haack analyses already focused on kind of single facts called needles placed in a Hy stack of Paul Graham essays um so I kind of extended that to kind of mirror the rag use case or kind of the rag context uh where I took multiple facts so I call it multi needle um I buil on a funny needle in the HTO challenge published by anthropic where they add they basically placed Pizza ingredients in the context uh and asked the LM to retrieve this combination of pizza ingredients I did I kind of Rift on that and I basically split the pizza ingredients up into three different needles and place those three ingredients in different places in the context and then ask the um to recover those three ingredients um from the context so again the setup is the question is what the secret ingredients need to build a perfect Pizza the needles are the ingredients figs Pudo goat cheese um I place them in the context at some specified intervals the way this test works is you can basically set the percent of context you want to place the first needle and the remaining two are placed at roughly equal intervals in the remaining context after the first so that's kind of the way the test is set up now it's all open source by the way the link is below so needs are placed um you ask a question you promp L them with with kind of um with this context and the question and then produces the answer and now the the framework will grade the response both one are you know all are all the the specified ingredients present in the answer and two if not which ones are missing so I ran a bunch of analysis on this with GPD 4 and came kind of came up with some with some fun results um so you can see on the left here what this is looking at is different numbers of needles placed in 120,000 token context window for gbd4 and I'm asking um gbd4 to retrieve either one three or 10 needles now I'm also asking it to do reasoning on those needles that's what you can see in those red bars so green is just retrieve the ingredients red is reasoning and the reasoning challenge here is just return the first letter of each ingredient so we find is basically two things the performance or the percentage of needles retrieved drops with respect to the number of needles that's kind of intuitive you place more facts performance gets worse but also it gets worse if you ask it to reason so if you say um just return the needles it does a little bit better than if you say return the needles and tell me the first letter so you overlay reasoning so this is the first observation more facts is harder uh and reasoning is harder uh than just retrieval now the second question we ask is where are these needles actually present in the context that we're missing right so we know for example um retrieval of um 10 needles is around 60% so where are the missing needles in the context so on the right you can see results telling us actually which specific needles uh are are the model fails to retrieve so what we can see is as you go from a th000 tokens up to 120,000 tokens on the X here and you look at needle one place at the start of the document to needle 10 placed at the end at a th000 token context link you can retrieve them all so again kind of match what we see over here small well actually sorry over here everything I'm looking at is 120,000 tokens so that's really not the point uh the point is actually smaller context uh better retrieval so that's kind of point one um as I increase the context window I actually see that uh there is increased failure to retrieve needles which you see can see in red here towards the start of the document um and so this is an interesting result um and it actually matches what Greg saw with single needle case as well so the way to think about it is it appears that um you know if you for example read a book and I asked you a question about the first chapter you might have forgotten it same kind of phenomenon appears to happen here with retrieval where needles towards the start of the context are are kind of Forgotten or are not well retrieved relative to those of the end so this is an effect we see with gbd4 it's been reproduced quite a bit so ran nine different trials here Greg's also seen this repeatedly with single needle so it seems like a pretty consistent result and there's an interesting point I put this on Twitter and a number of folks um you know replied and someone sent me this paper which is pretty interesting and it mentions recency bias is one possible reason so the most informative tokens for predicting the next token uh you know are are are present close to or recent to kind of where you're doing your generation and so there's a bias to attend to recent tokens which is obviously not great for the retrieval problem as we saw here so again the results show us that um reasoning is a bit harder than retrieval more needles is more difficult and needles towards the start of your context are harder to retrieve than towards the end those are three main observations from this and it maybe indeed due to this recency bias so overall what this kind of tells you is be wary of just context stuffing in large long context there are no retrieval guarantees and also there's some recent results that came out actually just today suggesting that single needle may be misleadingly easy um you know there's no reasoning it's retrieving a single needle um and also these guys I'm I showed this tweet here show that um the in a lot of these needle and Haack challenges including mine the facts that we look for are very different than um the background kind of Hy stack of Paul Graham essays and so that may be kind of an interesting artifact they note that indeed if the needle is more subtle retrievals is worse so I think basically when you see these really strong performing needle and hyack analyses put up by model providers you should be skeptical um you shouldn't necessarily assume that you're going to get high quality retrieval from these long contact LMS uh for numerous reasons you need to think about retrieval of multiple facts um you need to think about reasoning on top of retrieval you need need to think about the subtlety of the retrieval relative to the background context because for many of these needle and the Haack challenges it's a single needle no reasoning and the needle itself is very different from the background so anyway those may all make the challenge a bit easier than a real world scenario of fact retrieval so I just want to like kind of lay out that those cautionary notes but you know I think it is fair to say this will certainly get better and I think it's also fair to say that rag will change and this is just like a nearly not a great joke but Frank zap a musician made the point Jazz isn't dead it just smells funny you know I think same for rag rag is not dead but it will change I think that's like kind of the key Point here um so just as a followup on that rag today's focus on precise retrieval of relevant doc chunks so it's very focused on typically taking documents chunking them in some particular way often using very OS syncratic chunking methods things like chunk size are kind of picked almost arbitrarily embeding them storing them in an index taking a question embedding it doing K&N uh similarity search to retrieve relevant chunks you're often setting a k parameter which is the number of chunks you retrieve you often will do some kind of filtering or Pro processing on the retrieve chunks and then ground your answer in those retrieved chunks so it's very focused on precise retrieval of just the right chunks now in a world where you have very long context models I think there's a fair question to ask is is this really kind of the most most reasonable approach so kind of on the left here you can kind of see this notion closer to today of I need the exact relevant chunk you can risk over engineering you can have you know higher complexity sensitivity to these odd parameters like chunk size k um and you can indeed suffer lower recall because you're really only picking very precise chunks you're beholden to very particular embedding models so you know I think going forward as long context models get better and better there are definitely question you should certainly question the current kind of very precise chunking rag Paradigm but on the flip side I think just throwing all your docs into context probably will also not be the preferred approach you'll suffer higher latency higher token usage I should note that today 100,000 token GPD 4 is like $1 per generation I spent a lot of money on Lang Chain's account uh on that multile analysis I don't want to tell Harrison how much I spent uh so it's it's you know it's not good right um You Can't audit retrieve um", "type": "Document"}}