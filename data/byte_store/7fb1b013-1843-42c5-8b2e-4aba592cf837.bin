{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source_url": "https://www.youtube.com/watch?v=sVcwVQRHIc8", "type": "chunk", "doc_id": "7fb1b013-1843-42c5-8b2e-4aba592cf837", "original_doc_id": "2934a702-1b8a-4c62-a8dc-5dad86053b47"}, "page_content": "in this course Lance Martin will teach you how to implement rag from scratch Lance is a software engineer at Lang chain and Lang chain is one of the most common ways to implement rag Lance will help you understand how to use rag to combine custom data with llms hi this is Lance Martin I'm a software engineer at Lang chain I'm going to be giving a short course focused on rag or retrieval augmented generation which is one of the most popular kind of ideas and in llms today so really the motivation for this is that most of the world's data is private um whereas llms are trained on publicly available data so you can kind of see on the bottom on the x-axis the number of tokens using pre-training various llms so it kind of varies from say 1.5 trillion tokens in the case of smaller models like 52 out to some very large number that we actually don't know for proprietary models like GPT 4 CLA three but what's really interesting is that the context window or the ability to feed external information into these LMS is actually getting larger so about a year ago context windows were between 4 and 8,000 tokens you know that's like maybe a dozen pages of text we've recently seen models all the way out to a million tokens which is thousands of pages of text so while these llms are trained on large scale public data it's increasingly feasible to feed them this huge mass of private data that they've never seen that private data can be your kind of personal data it can be corporate data or you know other information that you want to pass to an LM that's not natively in his training set and so this is kind of the main motivation for rag it's really the idea that llms one are kind of the the center of a new kind of operating system and two it's increasingly critical to be able to feed information from external sources such as private data into llms for processing so that's kind of the overarching motivation for Rag and now rag refers to retrieval augmented generation and you can think of it in three very general steps there's a process of indexing of external data so you can think about this as you know building a database for example um many companies already have large scale databases in different forms they could be SQL DBS relational DBS um they could be Vector Stores um or otherwise but the point is that documents are indexed such that they can be retrieved based upon some heuristics relative to an input like a question and those relevant documents can be passed to an llm and the llm can produce answers that are grounded in that retrieved information so that's kind of the centerpiece or central idea behind Rag and why it's really powerful technology because it's really uniting the the knowledge and processing capacity of llms with large scale private external data source for which most of the important data in the world still lives and in the following short videos we're going to kind of build up a complete understanding of the rag landscape and we're going to be covering a bunch of interesting papers and techniques that explain kind of how to do rag and I've really broken it down into a few different sections so starting with a question on the left the first kind of section is what I call query trans translation so this captures a bunch of different methods to take a question from a user and modify it in some way to make it better suited for retrieval from you know one of these indexes we've talked about that can use methods like query writing it can be decomposing the query into you know constituent sub questions then there's a question of routing so taking that decomposed a Rewritten question and routing it to the right place you might have multiple Vector stores a relational DB graph DB and a vector store so it's the challenge of getting a question to the right Source then there's a there's kind of the challenge of query construction which is basically taking natural language and converting it into the DSL necessary for whatever data source you want to work with a classic example here is text a SQL which is kind of a very kind of well studied process but text a cipher for graph DV is very interesting text to metadata filters for Vector DBS is also a very big area of study um then there's indexing so that's the process of taking your documents and processing them in some way so they can be easily retrieved and there's a bunch of techniques for that we'll talk through we'll talk through different embedding methods we'll talk about different indexing strategies after retrieval there are different techniques to rerank or filter retrieve documents um and then finally we'll talk about generation and kind of an interesting new set of methods to do what we might call as active rag so in that retrieval or generation stage grade documents grade answers um grade for relevance to the question grade for faithfulness to the documents I.E check for hallucinations and if either fail feedback uh re- retrieve or rewrite the question uh regenerate the qu regenerate the answer and so forth so there's a really interesting set of methods we're going to talk through that cover that like retrieval and generation with feedback and you know in terms of General outline we'll cover the basics first it'll go through indexing retrieval and generation kind of in the Bare Bones and then we'll talk through more advanced techniques that we just saw on the prior slide career Transformations routing uh construction and so forth hi this is Lance from Lang chain this the second video in our series rack from scratch focused on indexing so in the past video you saw the main kind of overall components of rag pipelines indexing retrieval and generation and here we're going to kind of Deep dive on indexing and give like just a quick overview of it so the first aspect of indexing is we have some external documents that we actually want to load and put into what we're trying to call Retriever and the goal of this retriever is simply given an input question I want to fish out doents that are related to my question in some way now the way to establish that relationship or relevance or similarity is typically done using some kind of numerical representation of documents and the reason is that it's very easy to compare vectors for example of numbers uh relative to you know just free form text and so a lot of approaches have been a developed over the years to take text documents and compress them down into a numerical rep presentation that then can be very easily searched now there's a few ways to do that so Google and others came up with many interesting statistical methods where you take a document you look at the frequency of words and you build what they call sparse vectors such that the vector locations are you know a large vocabulary of possible words each value represents the number of occurrences of that particular word and it's sparse because there's of course many zeros it's a very large vocabulary relative to what's present in the document and there's very good search methods over this this type of numerical representation now a bit more recently uh embedding methods that are machine learned so you take a document and you build a compressed fixed length representation of that document um have been developed with correspondingly very strong search methods over embeddings um so the intuition here is that we take documents and we typically split them because embedding models actually have limited context windows so you know on the order of maybe 512 tokens up to 8,000 tokens or Beyond but they're not infinitely large so documents are split and each document is compressed into a vector and that Vector captures a semantic meaning of the document itself the vectors are indexed questions can be embedded in the exactly same way and then numerical kind of comparison in some form you know using very different types of methods can be performed on these vectors to fish out relevant documents relative to my question um and let's just do a quick code walk through on some of these points so I have my notebook here I've installed here um now I've set a few API keys for lsmith which are very useful for tracing which we'll see shortly um previously I walked through this this kind of quick start that just showed overall how to lay out these rag pipelines and here what I'll do is I'll Deep dive a little bit more on indexing and I'm going to take a question and a document and first I'm just going to compute the number of tokens in for example the question and this is interesting because embedding models in llms more generally operate on tokens and so it's kind of nice to understand how large the documents are that I'm trying to feed in in this case it's obviously a very small in this case question now I'm going to specify open eye embeddings I specify an embedding model here and I just say embed embed query I can pass my question my document and what you can see here is that runs and this is mapped to now a vector of length 1536 and that fixed length Vector representation will be computed for both documents and really for any document so you're always is kind of computing this fix length Vector that encodes the semantics of the text that you've passed now I can do things like cosine similarity to compare them and as we'll see here I can load some documents this is just like we saw previously I can split them and I can index them here just like we did before but we can see under the hood really what we're doing is we're taking each split we're embedding it using open eye embeddings into this this kind of this Vector representation and that's stored with a link to the rod document itself in our Vector store and next we'll see how to actually do retrieval using this Vector store hi this is Lance from Lang chain and this is the third video in our series rag from scratch building up a lot of the motivations for rag uh from the very basic components um so we're going to be talking about retrieval today in the last two uh short videos I outlined indexing and gave kind of an overview of this flow which starts with indexing of our documents retrieval of documents relevant to our question and then generation of answers based on the retriev documents and so we saw that the indexing process basically makes documents easy to retrieve and it goes through a flow that basically looks like you take our documents you split them in some way into these smaller chunks that can be easily embedded um those embeddings are then numerical representations of those documents that are easily searchable and they're stored in an index when given a question that's also embedded the index performs a similarity search and returns splits that are relevant to the question now if we dig a little bit more under the hood we can think about it like this if we take a document and embed it let's imagine that embedding just had three dimensions so you know each document is projected into some point in this 3D space now the point is that the location in space is determined by the semantic meaning or content in that document so to follow that then documents in similar locations in space contain similar semantic information and this very simple idea is really the Cornerstone for a lot of search and retrieval methods that you'll see with modern Vector stores so in particular we take our documents we embed them into this in this case a toy 3D space we take our question do the same we can then do a search like a local neighborhood search you can think about in this 3D space around our question to say hey what documents are nearby and these nearby neighbors are then retrieved because they can they have similar semantics relative to our question and that's really what's going on here so again we took our documents we split them we embed them and now they exist in this high dimensional space we've taken our question embedded it projected in that same space and we just do a search around the question from nearby documents and grab ones that are close and we can pick some number we can say we want one or two or three or n documents close to my question in this embedding space and there's a lot of really interesting methods that implement this very effectively I I link one here um and we have a lot of really nice uh Integrations to play with this general idea so many different embedding models many different indexes lots of document loaders um and lots of Splitters that can be kind of recombined to test different ways of doing this kind of indexing or retrieval um so now I'll show a bit of a code walkth through so here we defined um we kind of had walked through this previously this is our notebook we've installed a few packages we've set a few environment variables using lsmith and we showed this previously this is just an overview showing how to run rag like kind of end to end in the last uh short talk we went through indexing um and what I'm going to do very simply is I'm just going to reload our documents so now I have our documents I'm going to resplit them and we saw before how we can build our index now here let's actually do the same thing but in the slide we actually showed kind of that notion of search in that 3D space and a nice parameter to think about in building your your retriever is K so K tells you the number of nearby neighbors to fetch when you do that retrieval process and we talked about you know in that 3D space do I want one nearby neighbor or two or three so here we can specify k equals 1 for example now we're building our index so we're taking every split embedding it storing it now what's nice is I asked a a question what is Task decomposition this is related to the blog post and I'm going to run get relevant documents so I run that and now how many documents do I get back I get one as expected based upon k equals 1 so this retrieve document should be related to my question now I can go to lsmith and we can open it up and we can look at our Retriever and we can see here was our question here's the one document we got back and okay so that makes sense this document pertains to task ke decomposition in particular and it kind of lays out a number of different approaches that can be used to do that this all kind of makes sense and this shows kind of in practice how you can implement this this NE this kind of KNN or k nearest neighbor search uh really easily uh just using a few lines of code and next we're going to talk about generation thanks hey this is Lance from Lang chain this is the fourth uh short video in our rack from scratch series that's going to be focused on generation now in the past few videos we walked through the general flow uh for kind of basic rag starting with indexing Fall by retrieval then generation of an answer based upon the documents that we retrieved that are relevant to our question this is kind of the the very basic flow now an important consideration in generation is really what's happening is we're taking the documents you retrieve and we're stuffing them into the llm context window so if we kind of walk back through the process we take documents we split them for convenience or embedding we then embed each split and we store that in a vector store as this kind of easily searchable numerical representation or vector and we take a question embed it to produce a similar kind of numerical representation we can then search for example using something like KN andn in this kind of dimensional space for documents that are similar to our question based on their proximity or location in this space in this case you can see 3D is a toy kind of toy example now we've recovered relevant splits to our question we pack those into the context window and we produce our answer now this introduces the notion of a prompt so the prompt is kind of a you can think have a placeholder that has for example you know in our case B keys so those keys can be like context and question so they basically are like buckets that we're going to take those retrieve documents and Slot them in we're going to take our question and also slot it in and if you kind of walk through this flow you can kind of see that we can build like a dictionary from our retrieve documents and from our question and then we can basically populate our prompt template with the values from the dict and then becomes a prompt value which can be passed to llm like a chat model resulting in chat messages which we then parse into a string and get our answer so that's like the basic workflow that we're going to see and let's just walk through that in code very quickly to kind of give you like a Hands-On intuition so we had our notebook we walk through previously install a few packages I'm setting a few lsmith environment variables we'll see it's it's nice for uh kind of observing and debugging our traces um previously we did this quick start we're going to skip that over um and what I will do is I'm going to build our retriever so again I'm going to take documents and load them uh and then I'm going to split them here we've kind of done this previously so I'll go through this kind of quickly and then we're going to embed them and store them in our index so now we have this retriever object here now I'm going to jump down here now here's where it's kind of fun this is the generation bit and you can see here I'm defining something new this is a prompt template and what my prompt template is something really simple it's just going to say answer the following question based on this context it's going to have this context variable and a question so now I'm building my prompt so great now I have this prompt let's define an llm I'll choose 35 now this introdu the notion of a chain so in Lang chain we have an expression language called L Cel Lang chain expression language which lets you really easily compose things like prompts LMS parsers retrievers and other things but the very simple kind of you know example here is just let's just take our prompt which you defined right here and connect it to an LM which you defined right here into this chain so there's our chain now all we're doing is we're invoking that chain so every L expression language chain has a few common methods like invoke bat stream in this case we just invoke it with a dict so context and question that maps to the expected Keys here in our template and so if we run invoke what we see is it's just going to execute that chain and we get our answer now if we zoom over to Langs Smith we should see that it's been populated so yeah we see a very simple runable sequence here was our document um and here's our output and here is our prompt answer the following question based on the context here's the document we passed in here is the question and then we get our answer so that's pretty nice um now there's a lot of other options for rag prompts I'll pull one in from our prompt tub this one's like kind of a popular prompt so it just like has a little bit more detail but you know it's the main the main intuition is the same um you're passing in documents you're asking them to reason about the documents given a question produce an answer and now here I'm going to find a rag chain which will automatically do the retrieval for us and all I have to do is specify here's my retriever which we defined before here's our question we which we invoke with the question gets passed through to the key question in our dict and it automatically will trigger the retriever which will return documents which get passed into our context so it's exactly what we did up here except before we did this manually and now um this is all kind of automated for us we pass that dick which is autop populated into our prompt llm out to parser now let invoke it and that should all just run and great we get an answer and we can look at the trace and we can see everything that happened so we can see our retriever was run these documents were retrieved they get passed into our LM and we get our final answer so this kind of the end of our overview um where we talked about I'll go back to the slide here quickly we talked about indexing retrieval and now generation and follow-up short videos we'll kind of dig into some of the more com complex or detailed", "type": "Document"}}